{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language level evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from notebook_init import notebook_init, notebook_imports\n",
    "\n",
    "notebook_init()\n",
    "notebook_imports()\n",
    "\n",
    "from src.bots.evaluation_bot import LanguageLevelEvaluationBot\n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    ('The author\\'s nuanced use of symbolism and intricate narrative structure elevate the novel, making it a captivating exploration of the human psyche', 'A1', 1),\n",
    "    ('I recently read an interesting article about the impact of technology on modern society.', 'A1', 2),\n",
    "    ('I like to read. My favourite book is Alice in Wonderland', 'A1', 3),\n",
    "    ('The author\\'s nuanced use of symbolism and intricate narrative structure elevate the novel, making it a captivating exploration of the human psyche', 'A2', 1),\n",
    "    ('I would have really enjoyed the boat ride, such a pitty it is cancelled.', 'A2', 2),\n",
    "    ('I enjoy reading books and watching movies in my free time.', 'A2', 3),\n",
    "    ('The author\\'s nuanced use of symbolism and intricate narrative structure elevate the novel, making it a captivating exploration of the human psyche','B1', 1),\n",
    "    ('The author\\'s eloquent use of language captivates readers and imbues the narrative with a profound sense of emotional depth.', 'B1', 2),\n",
    "    ('I like to read books and watch movies in my free time.', 'B1', 3),\n",
    "    ('I love my mom a lot.', 'B2', 1),\n",
    "    ('I like to read books and watch movies in my free time.', 'B2', 2),\n",
    "    ('I recently read an interesting article about climate change and its effects on marine life.', 'B2', 3),\n",
    "    ('I like to read a lot.', 'C1', 1),\n",
    "    ('The recent increase in online shopping has led to a significant shift in consumer behavior and preferences.', 'C1', 2),\n",
    "    ('The author\\'s eloquent use of language captivates readers and imbues the narrative with a profound sense of emotional depth.', 'C1', 3),\n",
    "    ('I like to read books and watch movies.', 'C2', 1),\n",
    "    ('I recently read an interesting article about climate change and its effects on marine life.', 'C2', 2),\n",
    "    ('His eloquent discourse on the complexities of quantum physics captivated even the most astute minds in the audience.', 'C2', 3)\n",
    "]\n",
    "\n",
    "correct_ratings = [e[2] for e in examples] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_diff(l1, l2):\n",
    "    return sum( 1 if abs(e[0] - e[1]) > 1 else 0 for e in zip(l1,l2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.5\n",
      "Num diff > 1:  0\n",
      "[1, 1, 2, 1, 2, 3, 1, 1, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3]\n",
      "[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "bot = LanguageLevelEvaluationBot(0)\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(0)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = LanguageLevelEvaluationBot(1)\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5555555555555555\n",
      "Average percentage diff > 1:  0.05\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(1)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.4444444444444444\n",
      "Num diff > 1:  0\n",
      "[1, 1, 3, 1, 2, 2, 2, 1, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3]\n",
      "[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "bot = LanguageLevelEvaluationBot(2)\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0], print_prompt=True) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5222222222222223\n",
      "Average percentage diff > 1:  0.09444444444444447\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(2)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = LanguageLevelEvaluationBot(3)\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0], print_prompt=True) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5277777777777779\n",
      "Average percentage diff > 1:  0.01111111111111111\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(3)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = LanguageLevelEvaluationBot(prompt_alternative=4, level_description_fname='../../templates/language_levels_cefr_extended.json')\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0], print_prompt=True) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5166666666666667\n",
      "Average percentage diff > 1:  0.005555555555555555\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(4)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = LanguageLevelEvaluationBot(prompt_alternative=5)\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0], print_prompt=True) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5111111111111111\n",
      "Average percentage diff > 1:  0.0\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(5)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "\n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = LanguageLevelEvaluationBot(6, level_description_fname='../../templates/language_levels_cefr.json')\n",
    "\n",
    "ratings = [ bot.evaluate('English', m[1], m[0], print_prompt=True) for m in examples]\n",
    "\n",
    "acc =  accuracy_score(ratings, correct_ratings)   \n",
    "diff_g_e = count_diff(ratings, correct_ratings)\n",
    "\n",
    "\n",
    "print('Acc: ', acc)\n",
    "print('Num diff > 1: ', diff_g_e )\n",
    "print(ratings)\n",
    "print(correct_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average acc:  0.5722222222222222\n",
      "Average percentage diff > 1:  0.05\n"
     ]
    }
   ],
   "source": [
    "sum_acc = 0\n",
    "sum_num_diff = 0\n",
    "n_runs = 10\n",
    "\n",
    "bot = LanguageLevelEvaluationBot(6)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "\n",
    "    ratings = [ bot.evaluate('English', m[1], m[0]) for m in examples]\n",
    "\n",
    "    sum_acc +=  accuracy_score(ratings, correct_ratings)   \n",
    "\n",
    "    sum_num_diff += count_diff(ratings, correct_ratings)/len(examples)\n",
    "    \n",
    "print('Average acc: ', sum_acc/n_runs)\n",
    "print('Average percentage diff > 1: ', sum_num_diff/n_runs )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Prompt alternative | Average accuracy | Average percentage of difference > 1 |\n",
    "|-------------------|------------------|--------------------------------------|\n",
    "| 0 | 0.522 | 0.0 |\n",
    "| 1 |  0.556 | 0.05 |\n",
    "| 2 | 0.522| 0.09 |\n",
    "| 3 | 0.528 |0.011|\n",
    "| 4 | 0.516 |0.001 |\n",
    "| 5 | 0.511 | 0.0 |\n",
    "| 6 | 0.572| 0.05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
